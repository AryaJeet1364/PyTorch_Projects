{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMKwmirAwBuddNfdxrwHgF7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AryaJeet1364/PyTorch_Projects/blob/main/TransformersfromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Attention is all you need - from scratch"
      ],
      "metadata": {
        "id": "9mL37HHlRiXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "Complete implementation of the Transformer architecture from the original paper by Vaswani et al. (2017), built from scratch using PyTorch for German-English translation.\n",
        "\n",
        "##Key Features\n",
        "\n",
        "✅ Multi-head self-attention mechanism\n",
        "\n",
        "✅ Positional encoding\n",
        "\n",
        "✅ Encoder-decoder architecture\n",
        "\n",
        "✅ Label smoothing and learning rate scheduling\n",
        "\n",
        "✅ Training on Multi30k dataset\n",
        "\n",
        "##Implementation Details\n",
        "\n",
        "Paper: Attention Is All You Need (Vaswani et al., 2017)\n",
        "\n",
        "Framework: PyTorch\n",
        "\n",
        "Dataset: Multi30k German-English translation\n",
        "\n",
        "Key Components: Custom attention, positional encoding, transformer blocks\n",
        "\n",
        "##Acknowledgments\n",
        "\n",
        "Implementation developed with guidance from the original paper, supplementary blogs, and AI assistance for debugging and optimization."
      ],
      "metadata": {
        "id": "bklNvYRWRGmu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irSh0g1ZKuEo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"GPU is NOT available\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K03E___qAsvM",
        "outputId": "f67dab12-d0c6-4528-a13d-bea7207aeddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # Get dimensions - query, key, value can have different sequence lengths\n",
        "        batch_size = query.size(0)\n",
        "        query_len = query.size(1)\n",
        "        key_len = key.size(1)\n",
        "        value_len = value.size(1)\n",
        "\n",
        "        # Apply linear transformations and reshape for multi-head attention\n",
        "        # Query: (batch_size, query_len, d_model) -> (batch_size, num_heads, query_len, d_k)\n",
        "        Q = self.W_q(query).view(batch_size, query_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Key: (batch_size, key_len, d_model) -> (batch_size, num_heads, key_len, d_k)\n",
        "        K = self.W_k(key).view(batch_size, key_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Value: (batch_size, value_len, d_model) -> (batch_size, num_heads, value_len, d_k)\n",
        "        V = self.W_v(value).view(batch_size, value_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Apply scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Concatenate heads and put through final linear layer\n",
        "        # (batch_size, num_heads, query_len, d_k) -> (batch_size, query_len, d_model)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, query_len, self.d_model\n",
        "        )\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.W_o(attn_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "D8vWKzdbVSNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This feed-forward network is applied independently to each position in the sequence\n",
        "# It has two linear layers with a ReLU activation in between\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "    # d_model: input and output dimensionality (same as the model dimension)\n",
        "    # d_ff: hidden layer dimensionality (usually larger than d_model)\n",
        "    # dropout: dropout probability for regularization\n",
        "\n",
        "    super().__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(d_model, d_ff),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout), # Dropout after activation to prevent overfitting\n",
        "        nn.Linear(d_ff, d_model),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "U5YyK42kDyKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional encoding gives the model information about the position of each token in the sequence.\n",
        "# Since the Transformer does not use recurrence or convolution, this is crucial for capturing order.\n",
        "\n",
        "#   PE(pos, 2i)   = sin(pos / (10000^(2i / d_model)))\n",
        "#   PE(pos, 2i+1) = cos(pos / (10000^(2i / d_model)))\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Creating a zero tensor of shape (max_seq_length, d_model) to hold/' the positional encodings\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "\n",
        "        # Creating a tensor containing position indices: [0, 1, 2, ..., max_seq_length - 1]\n",
        "        # Shape: (max_seq_length, 1)\n",
        "        position = torch.arange(0, max_seq_length).unsqueeze(1)\n",
        "\n",
        "        # Creating a tensor containing the denominator terms for sine and cosine functions\n",
        "        # Shape: (d_model // 2,)\n",
        "        # Each i corresponds to a dimension in the embedding\n",
        "        # The formula below implements:\n",
        "        #   PE(pos, 2i)   = sin(pos / (10000^(2i / d_model)))\n",
        "        #   PE(pos, 2i+1) = cos(pos / (10000^(2i / d_model)))\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        # Applying the sin function to even indices (dimensions 0, 2, 4, ...)\n",
        "        # Using: sin(pos / (10000^(2i/d_model)))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Applying the cos function to odd indices (dimensions 1, 3, 5, ...)\n",
        "        # Using: cos(pos / (10000^(2i/d_model)))\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Adding a batch dimension at the front to make shape (1, max_seq_length, d_model)\n",
        "        # So it can be easily added (broadcasted) to input embeddings of shape (batch_size, seq_len, d_model)\n",
        "        # register_buffer ensures 'pe' is part of model state (saved and loaded), but not a trainable parameter\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Adding positional encodings to the input embeddings\n",
        "        # Only take up to the length of the input sequence\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "vzqfMgqqIdfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-head self-attention layer:\n",
        "        # Allows the model to attend to different parts of the sequence simultaneously.\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "        # Layer normalization after self-attention block\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Position-wise Feed Forward Network:\n",
        "        # Applies two linear transformations with a ReLU in between to each token independently.\n",
        "        self.feed_forward = FeedForwardNetwork(d_model, d_ff)\n",
        "\n",
        "        # Layer normalization after feed-forward block\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout layer to prevent overfitting and add regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        # mask: Optional attention mask (used to ignore padding tokens or future tokens)\n",
        "\n",
        "        # ---- Self-Attention Block ----\n",
        "\n",
        "        # Apply multi-head self-attention:\n",
        "        # Query = Key = Value = x (since it's self-attention),\n",
        "        # allowing each token to attend to every other token in the sequence.\n",
        "        attn_output = self.self_attn(x, x, x, mask)  # Shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Apply dropout to attention output (for regularization)\n",
        "        attn_output = self.dropout(attn_output)\n",
        "\n",
        "        # Add residual connection: adds original input x to the attention output\n",
        "        # Apply layer normalization to stabilize and speed up training\n",
        "        x = self.norm1(x + attn_output)  # Shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # ---- Feed-Forward Network Block ----\n",
        "\n",
        "        # Apply feed-forward network:\n",
        "        # A two-layer MLP applied independently to each position/token.\n",
        "        ff_output = self.feed_forward(x)  # Shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Apply dropout to FFN output\n",
        "        ff_output = self.dropout(ff_output)\n",
        "\n",
        "        # Add residual connection again: input x + FFN output\n",
        "        # Apply second layer normalization\n",
        "        x = self.norm2(x + ff_output)  # Shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Return the final encoded representation of the input tokens\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "adX_HBOJNdfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Multi-head self-attention (with causal mask so decoder can't peek at future tokens)\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "        # Cross-attention: Decoder attends to encoder's output\n",
        "        # Query comes from decoder input; Key and Value come from encoder output\n",
        "        self.cross_attn = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "        # Position-wise feed-forward layer (applied to each position separately)\n",
        "        self.feed_forward = FeedForwardNetwork(d_model, d_ff)\n",
        "\n",
        "        # Layer Normalizations for each sub-layer (Post residual connection)\n",
        "        self.norm1 = nn.LayerNorm(d_model)  # For self-attention\n",
        "        self.norm2 = nn.LayerNorm(d_model)  # For cross-attention\n",
        "        self.norm3 = nn.LayerNorm(d_model)  # For feed-forward\n",
        "\n",
        "        # Dropout layer to prevent overfitting\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Decoder input (batch_size, tgt_seq_len, d_model)\n",
        "            enc_output: Encoder output (batch_size, src_seq_len, d_model)\n",
        "            src_mask: Mask for encoder input (optional, usually for padding)\n",
        "            tgt_mask: Mask for decoder input (causal mask to prevent peeking ahead)\n",
        "        Returns:\n",
        "            x: Output of decoder layer (batch_size, tgt_seq_len, d_model)\n",
        "        \"\"\"\n",
        "\n",
        "        # ----- Step 1: Self-attention (with causal mask) -----\n",
        "        # Decoder attends to earlier positions in the output only\n",
        "        self_attn_output = self.self_attn(x, x, x, tgt_mask)  # Shape: (batch_size, tgt_seq_len, d_model)\n",
        "\n",
        "        # Add & Norm: residual connection followed by layer normalization\n",
        "        x = self.norm1(x + self.dropout(self_attn_output))\n",
        "\n",
        "        # ----- Step 2: Cross-attention -----\n",
        "        # Decoder attends to encoder output (source sentence)\n",
        "        # Q = current decoder state, K = V = encoder output\n",
        "        cross_attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "\n",
        "        # Add & Norm again\n",
        "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
        "\n",
        "        # ----- Step 3: Feed Forward -----\n",
        "        # Position-wise fully connected layers to each token\n",
        "        ff_output = self.feed_forward(x)\n",
        "\n",
        "        # Final residual and normalization\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "\n",
        "        # Output is passed to the next decoder layer or final linear + softmax\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "pLROt8bIKTqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,       # Size of the input vocabulary\n",
        "                 d_model,          # Dimensionality of embeddings and hidden states\n",
        "                 num_layers=6,     # Number of encoder layers to stack\n",
        "                 num_heads=8,      # Number of attention heads in each layer\n",
        "                 d_ff=2048,        # Hidden size of feedforward network\n",
        "                 dropout=0.1,      # Dropout rate\n",
        "                 max_seq_length=5000):  # Maximum sequence length (for positional encoding)\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Learnable token embeddings: maps each input token to a dense vector of size d_model\n",
        "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # 2. Scale factor as recommended in the original Transformer paper\n",
        "        # This prevents the dot-product values from being too small at the start of training\n",
        "        self.scale = math.sqrt(d_model)\n",
        "\n",
        "        # 3. Positional Encoding: Adds information about the position of tokens in the sequence\n",
        "        self.pe = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        # 4. Dropout: Helps prevent overfitting by randomly zeroing some values\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 5. Stack of Encoder Layers: Each layer contains self-attention and feedforward sublayers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tokens as indices (batch_size, seq_len)\n",
        "            mask: Optional attention mask to ignore padding tokens\n",
        "        Returns:\n",
        "            x: Encoded output (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Embed input token indices and scale\n",
        "        # Input shape: (batch_size, seq_len) → Output shape: (batch_size, seq_len, d_model)\n",
        "        x = self.embeddings(x) * self.scale\n",
        "\n",
        "        # Step 2: Add positional encoding and apply dropout\n",
        "        x = self.dropout(self.pe(x))\n",
        "\n",
        "        # Step 3: Pass through each encoder layer sequentially\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        # Output is a sequence of contextualized embeddings for each token\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "oOroIUsqKsUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,       # Vocabulary size for target tokens\n",
        "                 d_model,          # Embedding and hidden dimension size\n",
        "                 num_layers=6,     # Number of stacked decoder layers\n",
        "                 num_heads=8,      # Number of attention heads per layer\n",
        "                 d_ff=2048,        # Hidden size of feedforward network\n",
        "                 dropout=0.1,      # Dropout probability\n",
        "                 max_seq_length=5000):  # Maximum target sequence length for positional encoding\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Output token embeddings: map each target token to a dense vector\n",
        "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # 2. Scale embeddings by sqrt(d_model) to stabilize training as per the original Transformer paper\n",
        "        self.scale = math.sqrt(d_model)\n",
        "\n",
        "        # 3. Positional encoding to add sequence order information (since attention itself is order-agnostic)\n",
        "        self.pe = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        # 4. Dropout for regularization to prevent overfitting\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 5. Stack of N decoder layers (each with self-attention, cross-attention, and feedforward)\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Target input tokens (batch_size, target_seq_len)\n",
        "            encoder_output: Encoder output states (batch_size, source_seq_len, d_model)\n",
        "            src_mask: Optional mask for source padding tokens (to ignore in cross-attention)\n",
        "            tgt_mask: Optional mask for target tokens (to ignore padding and future tokens in self-attention)\n",
        "        Returns:\n",
        "            x: Output of decoder (batch_size, target_seq_len, d_model)\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Embed target tokens and scale\n",
        "        # Shape changes from (batch_size, target_seq_len) to (batch_size, target_seq_len, d_model)\n",
        "        x = self.embeddings(x) * self.scale\n",
        "\n",
        "        # Step 2: Add positional encoding and apply dropout\n",
        "        x = self.dropout(self.pe(x))\n",
        "\n",
        "        # Step 3: Pass through each decoder layer sequentially\n",
        "        # Each layer applies masked self-attention, encoder-decoder cross-attention, and feedforward sublayer\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        # Return the contextualized decoder embeddings\n",
        "        return x"
      ],
      "metadata": {
        "id": "PBJHaPBqL_Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace your existing mask functions with these corrected versions:\n",
        "\n",
        "def create_padding_mask(seq,pad_idx=2):\n",
        "    \"\"\"\n",
        "    Create mask for padding tokens (token ID 0)\n",
        "    Args:\n",
        "        seq: Input sequence tensor of shape (batch_size, seq_len)\n",
        "        pad_idx: Padding token index (2 for <blank> in your vocab)\n",
        "    Returns:\n",
        "        mask: Padding mask of shape (batch_size, 1, 1, seq_len)\n",
        "              Returns 1 for valid tokens, 0 for padding tokens\n",
        "    \"\"\"\n",
        "    batch_size, seq_len = seq.shape\n",
        "\n",
        "    # Create mask: 1 for non-padding tokens, 0 for padding tokens\n",
        "    mask = (seq != pad_idx).float()\n",
        "\n",
        "    # Reshape to (batch_size, 1, 1, seq_len) for broadcasting\n",
        "    return mask.view(batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "def create_future_mask(size, device):\n",
        "    \"\"\"\n",
        "    Create mask to prevent attending to future tokens (causal mask)\n",
        "    Args:\n",
        "        size: length of the sequence (target_seq_len)\n",
        "        device: device to create the tensor on\n",
        "    Returns:\n",
        "        mask: Future mask of shape (1, 1, size, size)\n",
        "              Returns 1 for allowed positions, 0 for future positions\n",
        "    \"\"\"\n",
        "    # Create lower triangular matrix (including diagonal)\n",
        "    mask = torch.tril(torch.ones((size, size), device=device))\n",
        "\n",
        "    # Add batch and head dimensions: (1, 1, size, size)\n",
        "    return mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "\n",
        "def create_masks(src, tgt):\n",
        "    \"\"\"\n",
        "    Create source and target masks for Transformer training.\n",
        "    Args:\n",
        "        src: Source input sequence (batch_size, src_len)\n",
        "        tgt: Target input sequence (batch_size, tgt_len)\n",
        "    Returns:\n",
        "        src_mask: Padding mask for source input (batch_size, 1, 1, src_len)\n",
        "        tgt_mask: Combined padding and future mask for target input\n",
        "    \"\"\"\n",
        "    # Get device from input tensors\n",
        "    device = src.device\n",
        "\n",
        "    # 1. Create padding masks for source and target sequences\n",
        "    src_padding_mask = create_padding_mask(src)\n",
        "    tgt_padding_mask = create_padding_mask(tgt)\n",
        "\n",
        "    # 2. Create future mask to prevent attention to future tokens in target\n",
        "    tgt_len = tgt.size(1)\n",
        "    tgt_future_mask = create_future_mask(tgt_len, device)\n",
        "\n",
        "    # 3. Combine masks using element-wise multiplication (both should be 1 for valid positions)\n",
        "    tgt_mask = tgt_padding_mask * tgt_future_mask\n",
        "\n",
        "    return src_padding_mask, tgt_mask"
      ],
      "metadata": {
        "id": "xlNU5H-oME_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_vocab_size,   # Vocabulary size of the source language/input\n",
        "                 tgt_vocab_size,   # Vocabulary size of the target language/output\n",
        "                 d_model,          # Embedding and model dimension\n",
        "                 num_layers=6,     # Number of encoder and decoder layers (transformer blocks)\n",
        "                 num_heads=8,      # Number of attention heads in multi-head attention\n",
        "                 d_ff=2048,        # Dimension of feed-forward layer inside transformer blocks\n",
        "                 dropout=0.1,      # Dropout rate for regularization\n",
        "                 max_seq_length=5000):  # Maximum sequence length for positional encoding\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize the Encoder with parameters\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size,\n",
        "            d_model,\n",
        "            num_layers,\n",
        "            num_heads,\n",
        "            d_ff,\n",
        "            dropout,\n",
        "            max_seq_length\n",
        "        )\n",
        "\n",
        "        # Initialize the Decoder with parameters\n",
        "        self.decoder = Decoder(\n",
        "            tgt_vocab_size,\n",
        "            d_model,\n",
        "            num_layers,\n",
        "            num_heads,\n",
        "            d_ff,\n",
        "            dropout,\n",
        "            max_seq_length\n",
        "        )\n",
        "\n",
        "        # Final linear layer to project decoder output to target vocab probabilities\n",
        "        self.final_layer = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the Transformer model\n",
        "        Args:\n",
        "            src: Input source sequence tensor of shape (batch_size, src_seq_len)\n",
        "            tgt: Input target sequence tensor of shape (batch_size, tgt_seq_len)\n",
        "            src_mask: Optional mask for source sequence\n",
        "            tgt_mask: Optional mask for target sequence\n",
        "        Returns:\n",
        "            output: Raw logits for each target token (batch_size, tgt_seq_len, tgt_vocab_size)\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Create masks for padding and future tokens\n",
        "        # src_mask masks padded tokens in source input for encoder attention\n",
        "        # tgt_mask combines padding and causal mask for decoder attention\n",
        "        if src_mask is None or tgt_mask is None:\n",
        "            src_mask, tgt_mask = create_masks(src, tgt)\n",
        "\n",
        "        # 2. Pass the source sequence through the encoder\n",
        "        # encoder_output shape: (batch_size, src_seq_len, d_model)\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "\n",
        "        # 3. Pass the target sequence and encoder output to the decoder\n",
        "        # decoder_output shape: (batch_size, tgt_seq_len, d_model)\n",
        "        decoder_output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        # 4. Project the decoder output to logits over the target vocabulary\n",
        "        output = self.final_layer(decoder_output)\n",
        "\n",
        "        # 5. Note:\n",
        "        # We usually don't apply softmax here because the loss function\n",
        "        # like nn.CrossEntropyLoss expects raw logits and applies log_softmax internally.\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "xCuMWqy7MWm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLRScheduler():\n",
        "    def __init__(self, optimizer, d_model, warmup_steps):\n",
        "        \"\"\"\n",
        "        Learning rate scheduler for Transformer (based on the original paper)\n",
        "\n",
        "        Args:\n",
        "            optimizer: Optimizer whose learning rate will be updated\n",
        "            d_model: Model embedding size (used for scaling)\n",
        "            warmup_steps: Number of steps to linearly increase the learning rate (warm-up phase)\n",
        "        \"\"\"\n",
        "        self.optimizer = optimizer\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.step_num = 0  # Track step number internally\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Update the learning rate based on current step number\n",
        "\n",
        "        Formula:\n",
        "            lrate = d_model^(-0.5) * min(step_num^(-0.5), step_num * warmup_steps^(-1.5))\n",
        "\n",
        "        This means:\n",
        "          - Initially (during warmup), learning rate increases linearly with step_num\n",
        "          - After warmup, learning rate decreases proportionally to step_num^(-0.5)\n",
        "\n",
        "        Args:\n",
        "            step_num: Current training step (int or float)\n",
        "        \"\"\"\n",
        "        self.step_num += 1\n",
        "        # Calculate learning rate scalar using the formula\n",
        "        lrate = (self.d_model ** -0.5) * min(\n",
        "            self.step_num ** -0.5,\n",
        "            self.step_num * (self.warmup_steps ** -1.5)\n",
        "        )\n",
        "\n",
        "        # Update optimizer learning rate for all parameter groups\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lrate\n",
        "\n",
        "        return lrate"
      ],
      "metadata": {
        "id": "LvCrDdySMwL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        \"\"\"\n",
        "        Implements label smoothing regularization\n",
        "\n",
        "        Args:\n",
        "            smoothing: Float, amount of smoothing to apply (typically 0.1)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1.0 - smoothing  # Probability for the correct class\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        \"\"\"\n",
        "        Calculate the label-smoothed loss\n",
        "\n",
        "        Args:\n",
        "            logits: Model outputs before softmax (batch_size, vocab_size)\n",
        "            target: True class indices (batch_size)\n",
        "\n",
        "        Returns:\n",
        "            loss: Smoothed cross-entropy loss (scalar)\n",
        "        \"\"\"\n",
        "        vocab_size = logits.size(-1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Create a tensor of the same shape as logits filled with smoothing values\n",
        "            true_dist = torch.zeros_like(logits)\n",
        "            # Distribute smoothing mass to all classes except the true class\n",
        "            true_dist.fill_(self.smoothing / (vocab_size - 1))\n",
        "            # Assign confidence (1 - smoothing) probability to the true class index\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "\n",
        "        # Compute cross-entropy between smoothed true distribution and predicted logits\n",
        "        loss = torch.mean(torch.sum(-true_dist * torch.log_softmax(logits, dim=-1), dim=-1))\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "pQiwX5aZM7tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paVOrIAjCZj2",
        "outputId": "24e9113e-df62-4c8e-85fe-b45865f1e98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDN4fsHXTooa",
        "outputId": "035e300a-8bbd-4956-ba8c-42160589eac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transformer(model, train_dataloader, criterion, optimizer, scheduler, num_epochs, device='cuda'):\n",
        "    \"\"\"\n",
        "    Training loop for transformer\n",
        "\n",
        "    Args:\n",
        "        model: Transformer model\n",
        "        train_dataloader: DataLoader for training data\n",
        "        criterion: Loss function (with label smoothing)\n",
        "        optimizer: Optimizer\n",
        "        scheduler: Learning rate scheduler\n",
        "        num_epochs: Number of training epochs\n",
        "    \"\"\"\n",
        "    # 1. Setup\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # For tracking training progress\n",
        "    # total_loss = 0\n",
        "    all_losses = []\n",
        "\n",
        "    # 2. Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_dataloader):\n",
        "            # Get source and target batches\n",
        "            src = batch['src'].to(device)\n",
        "            tgt = batch['tgt'].to(device)\n",
        "\n",
        "            # Prepare target for input and output\n",
        "            # Remove last token from target for input\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            # Remove first token from target for output\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            # Create masks\n",
        "            src_mask, tgt_mask = create_masks(src, tgt_input)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(src, tgt_input, src_mask, tgt_mask)\n",
        "\n",
        "            # Reshape outputs and target for loss calculation\n",
        "            outputs = outputs.reshape(-1, outputs.size(-1))\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "            # Mask out padding tokens in loss calculation\n",
        "            # mask = (tgt_output != 2).float()  # Don't compute loss on padding tokens\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, tgt_output)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Update loss tracking\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Print progress every N batches\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Calculate average loss for epoch\n",
        "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "        all_losses.append(avg_epoch_loss)\n",
        "        print(f\"Epoch {epoch + 1} Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_epoch_loss,\n",
        "        }, f'checkpoint_epoch_{epoch+1}.pt')\n",
        "\n",
        "    return all_losses"
      ],
      "metadata": {
        "id": "AxH3DcmbVcfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import spacy\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def download_multi30k():\n",
        "    \"\"\"Download Multi30k dataset if not present\"\"\"\n",
        "    # Create data directory\n",
        "    if not os.path.exists('data'):\n",
        "        os.makedirs('data')\n",
        "\n",
        "    # Download files if they don't exist\n",
        "    base_url = \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/\"\n",
        "    files = {\n",
        "        \"train.de\": \"train.de.gz\",\n",
        "        \"train.en\": \"train.en.gz\",\n",
        "        \"val.de\": \"val.de.gz\",\n",
        "        \"val.en\": \"val.en.gz\",\n",
        "        \"test.de\": \"test_2016_flickr.de.gz\",\n",
        "        \"test.en\": \"test_2016_flickr.en.gz\"\n",
        "    }\n",
        "\n",
        "    for local_name, remote_name in files.items():\n",
        "        filepath = f'data/{local_name}'\n",
        "        if not os.path.exists(filepath):\n",
        "            url = base_url + remote_name\n",
        "            urllib.request.urlretrieve(url, filepath + '.gz')\n",
        "            os.system(f'gunzip -f {filepath}.gz')\n",
        "\n",
        "def load_data(filename):\n",
        "    \"\"\"Load data from file\"\"\"\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        return [line.strip() for line in f]\n",
        "\n",
        "def create_dataset():\n",
        "    \"\"\"Create dataset from files\"\"\"\n",
        "    # Download data if needed\n",
        "    download_multi30k()\n",
        "\n",
        "    # Load data\n",
        "    train_de = load_data('data/train.de')\n",
        "    train_en = load_data('data/train.en')\n",
        "    val_de = load_data('data/val.de')\n",
        "    val_en = load_data('data/val.en')\n",
        "\n",
        "    return (train_de, train_en), (val_de, val_en)\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_texts, tgt_texts, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer):\n",
        "        self.src_texts = src_texts\n",
        "        self.tgt_texts = tgt_texts\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = self.src_texts[idx]\n",
        "        tgt_text = self.tgt_texts[idx]\n",
        "\n",
        "        # Tokenize\n",
        "        src_tokens = [tok.text for tok in self.src_tokenizer(src_text)]\n",
        "        tgt_tokens = [tok.text for tok in self.tgt_tokenizer(tgt_text)]\n",
        "\n",
        "        unk_src = self.src_vocab[\"<unk>\"]\n",
        "        unk_tgt = self.tgt_vocab[\"<unk>\"]\n",
        "\n",
        "        src_indices = [self.src_vocab[\"<s>\"]] + [self.src_vocab.get(token, unk_src) for token in src_tokens] + [self.src_vocab[\"</s>\"]]\n",
        "        tgt_indices = [self.tgt_vocab[\"<s>\"]] + [self.tgt_vocab.get(token, unk_tgt) for token in tgt_tokens] + [self.tgt_vocab[\"</s>\"]]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'src': torch.tensor(src_indices),\n",
        "            'tgt': torch.tensor(tgt_indices)\n",
        "        }\n",
        "\n",
        "def build_vocab_from_texts(texts, tokenizer, min_freq=2):\n",
        "    \"\"\"Build vocabulary from texts\"\"\"\n",
        "    counter = {}\n",
        "    for text in texts:\n",
        "        for token in [tok.text for tok in tokenizer(text)]:\n",
        "            counter[token] = counter.get(token, 0) + 1\n",
        "\n",
        "    # Create vocabulary\n",
        "    vocab = {\"<s>\": 0, \"</s>\": 1, \"<blank>\": 2, \"<unk>\": 3}\n",
        "    idx = 4\n",
        "    for word, freq in counter.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = idx\n",
        "            idx += 1\n",
        "    return vocab\n",
        "\n",
        "def collate_batch(batch):\n",
        "    src_tensors = [item['src'] for item in batch]\n",
        "    tgt_tensors = [item['tgt'] for item in batch]\n",
        "\n",
        "    # Pad sequences\n",
        "    src_padded = torch.nn.utils.rnn.pad_sequence(src_tensors, batch_first=True, padding_value=2)\n",
        "    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_tensors, batch_first=True, padding_value=2)\n",
        "\n",
        "    return {\n",
        "        'src': src_padded,\n",
        "        'tgt': tgt_padded\n",
        "    }\n",
        "\n",
        "def create_dataloaders(batch_size=32):\n",
        "    # Load tokenizers\n",
        "    spacy_de = spacy.load(\"de_core_news_sm\")\n",
        "    spacy_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Get data\n",
        "    (train_de, train_en), (val_de, val_en) = create_dataset()\n",
        "\n",
        "    # Build vocabularies\n",
        "    vocab_src = build_vocab_from_texts(train_de, spacy_de)\n",
        "    vocab_tgt = build_vocab_from_texts(train_en, spacy_en)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TranslationDataset(\n",
        "        train_de, train_en,\n",
        "        vocab_src, vocab_tgt,\n",
        "        spacy_de, spacy_en\n",
        "    )\n",
        "\n",
        "    val_dataset = TranslationDataset(\n",
        "        val_de, val_en,\n",
        "        vocab_src, vocab_tgt,\n",
        "        spacy_de, spacy_en\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_batch\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_batch\n",
        "    )\n",
        "\n",
        "    return train_dataloader, val_dataloader, vocab_src, vocab_tgt"
      ],
      "metadata": {
        "id": "5dEQc7OHTQi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    train_dataloader, val_dataloader, vocab_src, vocab_tgt = create_dataloaders()\n",
        "\n",
        "    if train_dataloader is None:\n",
        "        print(\"Failed to load data. Please install spacy models first.\")\n",
        "        exit(1)\n",
        "\n",
        "    print(f\"Source vocab size: {len(vocab_src)}\")\n",
        "    print(f\"Target vocab size: {len(vocab_tgt)}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = Transformer(\n",
        "        src_vocab_size=len(vocab_src),\n",
        "        tgt_vocab_size=len(vocab_tgt),\n",
        "        d_model=512,\n",
        "        num_layers=6,\n",
        "        num_heads=8,\n",
        "        d_ff=2048,\n",
        "        dropout=0.1\n",
        "    ).to(device)\n",
        "\n",
        "    # Use CrossEntropyLoss with ignore_index for padding tokens\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=2)  # Ignore padding tokens\n",
        "\n",
        "    # You can also use your LabelSmoothing if preferred:\n",
        "    # criterion = LabelSmoothing(smoothing=0.1).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Use the custom TransformerLRScheduler\n",
        "    scheduler = TransformerLRScheduler(optimizer, d_model=512, warmup_steps=4000)\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    losses = train_transformer(\n",
        "        model=model,\n",
        "        train_dataloader=train_dataloader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        num_epochs=10,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    print(f\"Final losses: {losses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVZIpDBDTUX8",
        "outputId": "73d90b7e-20f5-4add-9ced-fb9c0c9eb4d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source vocab size: 8014\n",
            "Target vocab size: 6191\n",
            "Starting training...\n",
            "Epoch 1/10\n",
            "Batch 0, Loss: 9.0173\n",
            "Batch 100, Loss: 6.3632\n",
            "Batch 200, Loss: 5.0167\n",
            "Batch 300, Loss: 4.4785\n",
            "Batch 400, Loss: 4.1779\n",
            "Batch 500, Loss: 4.0542\n",
            "Batch 600, Loss: 3.7182\n",
            "Batch 700, Loss: 3.7322\n",
            "Batch 800, Loss: 3.4975\n",
            "Batch 900, Loss: 2.9767\n",
            "Epoch 1 Loss: 4.3983\n",
            "Epoch 2/10\n",
            "Batch 0, Loss: 3.1839\n",
            "Batch 100, Loss: 3.1761\n",
            "Batch 200, Loss: 2.9956\n",
            "Batch 300, Loss: 2.8564\n",
            "Batch 400, Loss: 2.6560\n",
            "Batch 500, Loss: 2.8358\n",
            "Batch 600, Loss: 2.6247\n",
            "Batch 700, Loss: 2.1188\n",
            "Batch 800, Loss: 2.8202\n",
            "Batch 900, Loss: 2.4986\n",
            "Epoch 2 Loss: 2.8344\n",
            "Epoch 3/10\n",
            "Batch 0, Loss: 2.2864\n",
            "Batch 100, Loss: 2.3400\n",
            "Batch 200, Loss: 2.8043\n",
            "Batch 300, Loss: 2.1772\n",
            "Batch 400, Loss: 2.2822\n",
            "Batch 500, Loss: 2.2163\n",
            "Batch 600, Loss: 2.3084\n",
            "Batch 700, Loss: 2.4858\n",
            "Batch 800, Loss: 2.5319\n",
            "Batch 900, Loss: 2.4284\n",
            "Epoch 3 Loss: 2.3629\n",
            "Epoch 4/10\n",
            "Batch 0, Loss: 1.7197\n",
            "Batch 100, Loss: 2.0489\n",
            "Batch 200, Loss: 1.9925\n",
            "Batch 300, Loss: 2.1243\n",
            "Batch 400, Loss: 2.0331\n",
            "Batch 500, Loss: 2.6569\n",
            "Batch 600, Loss: 2.1123\n",
            "Batch 700, Loss: 2.5890\n",
            "Batch 800, Loss: 2.0106\n",
            "Batch 900, Loss: 2.3361\n",
            "Epoch 4 Loss: 2.1801\n",
            "Epoch 5/10\n",
            "Batch 0, Loss: 1.8369\n",
            "Batch 100, Loss: 2.0645\n",
            "Batch 200, Loss: 2.2058\n",
            "Batch 300, Loss: 2.2633\n",
            "Batch 400, Loss: 2.1569\n",
            "Batch 500, Loss: 2.1822\n",
            "Batch 600, Loss: 2.0554\n",
            "Batch 700, Loss: 2.0397\n",
            "Batch 800, Loss: 2.3835\n",
            "Batch 900, Loss: 2.0079\n",
            "Epoch 5 Loss: 2.1652\n",
            "Epoch 6/10\n",
            "Batch 0, Loss: 2.0649\n",
            "Batch 100, Loss: 2.3486\n",
            "Batch 200, Loss: 1.8081\n",
            "Batch 300, Loss: 2.2118\n",
            "Batch 400, Loss: 2.4682\n",
            "Batch 500, Loss: 2.0233\n",
            "Batch 600, Loss: 1.9753\n",
            "Batch 700, Loss: 2.0927\n",
            "Batch 800, Loss: 2.3368\n",
            "Batch 900, Loss: 2.0558\n",
            "Epoch 6 Loss: 2.0837\n",
            "Epoch 7/10\n",
            "Batch 0, Loss: 1.8853\n",
            "Batch 100, Loss: 2.3348\n",
            "Batch 200, Loss: 2.0745\n",
            "Batch 300, Loss: 1.8140\n",
            "Batch 400, Loss: 1.9994\n",
            "Batch 500, Loss: 1.9625\n",
            "Batch 600, Loss: 2.1023\n",
            "Batch 700, Loss: 2.1257\n",
            "Batch 800, Loss: 2.1651\n",
            "Batch 900, Loss: 1.9771\n",
            "Epoch 7 Loss: 1.9704\n",
            "Epoch 8/10\n",
            "Batch 0, Loss: 1.8825\n",
            "Batch 100, Loss: 1.8157\n",
            "Batch 200, Loss: 1.8130\n",
            "Batch 300, Loss: 2.0444\n",
            "Batch 400, Loss: 1.7341\n",
            "Batch 500, Loss: 1.5994\n",
            "Batch 600, Loss: 1.7273\n",
            "Batch 700, Loss: 1.8906\n",
            "Batch 800, Loss: 1.8372\n",
            "Batch 900, Loss: 2.2249\n",
            "Epoch 8 Loss: 1.8748\n",
            "Epoch 9/10\n",
            "Batch 0, Loss: 1.7649\n",
            "Batch 100, Loss: 1.7584\n",
            "Batch 200, Loss: 1.7288\n",
            "Batch 300, Loss: 2.1011\n",
            "Batch 400, Loss: 1.6667\n",
            "Batch 500, Loss: 1.8560\n",
            "Batch 600, Loss: 1.8692\n",
            "Batch 700, Loss: 2.0095\n",
            "Batch 800, Loss: 1.6440\n",
            "Batch 900, Loss: 1.9895\n",
            "Epoch 9 Loss: 1.7621\n",
            "Epoch 10/10\n",
            "Batch 0, Loss: 1.7511\n",
            "Batch 100, Loss: 1.6611\n",
            "Batch 200, Loss: 1.7887\n",
            "Batch 300, Loss: 1.5889\n",
            "Batch 400, Loss: 1.6492\n",
            "Batch 500, Loss: 1.7036\n",
            "Batch 600, Loss: 1.7051\n",
            "Batch 700, Loss: 1.6273\n",
            "Batch 800, Loss: 1.8010\n",
            "Batch 900, Loss: 1.8082\n",
            "Epoch 10 Loss: 1.6449\n",
            "Training completed!\n",
            "Final losses: [4.398304021871077, 2.834367767584232, 2.3629103804370666, 2.1801154859273675, 2.1651854881772397, 2.0836838340548987, 1.9703713040367639, 1.8747779864329093, 1.7620836336005483, 1.6449251450935036]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-VsZ6FXYC_0a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}